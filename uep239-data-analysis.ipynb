{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94363f6-56ac-48e2-a77b-a01f473ce956",
   "metadata": {},
   "source": [
    "# UEP-0239: Python for Data Analysis and Visualization\n",
    "\n",
    "---\n",
    "\n",
    "**A Tufts University Data Lab Tutorial**  \n",
    "Written by Uku-Kaspar Uustalu\n",
    "\n",
    "Contact: <uku-kaspar.uustalu@tufts.edu>\n",
    "\n",
    "Last updated: `2022-03-01`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330a0d1-d7db-4d87-b8a7-f7fe3985332b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Importing Packages\n",
    "\n",
    "We will be using the following Python data analysis and visualization libraries throguhout this tutorial:\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/) is the primary data analysis library in Python. It allows for easy analysis and manipulation of tabular data and is usually imported under the alias `pd`.\n",
    "- [Matplotlib](https://matplotlib.org/) is the most essential data visualization library in Python. Although it consists of many modules, most of the plotting funcionality is contained within the `matplotlib.pyplot` module, which is usually imported under the alias `plt`.\n",
    "- [Seaborn](https://seaborn.pydata.org/) is an advanced plotting library that is built on top of Matplotlib. It has a simpler interface and allows for the easy creation of beutiful visualizations. Seaborn is usually imported under the alias `sns`.\n",
    "- [HVPlot](https://hvplot.holoviz.org/) is a high-level plotting interface that integrates seamlessly with Pandas and allows for the easy creation of interactive visualizations. The `hvplot.pandas` module must be imported to allow for seamless integration with Pandas.\n",
    "- [Plotly](https://plotly.com/) is an alternative interactive visualization library. It consists of many modules, but the `plotly.express` module is the easiest to use as it allows for the creation of whole plots using a single command. The module is usually imported under the alias `px`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199e6cd-77ea-4d60-808e-8831582b6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hvplot.pandas\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea8562-9b46-46a1-97ef-51bfadf2f2bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Getting Started with Pandas\n",
    "\n",
    "For the first part of this tutorial, we will be using the following datasets from the `data` folder to investigate the relationship between wealth and health:\n",
    "\n",
    "- [`gdp.csv`](./data/gdp.csv) – World Bank gross domestic product (GDP) estimates (in USD) for world countries and regions from 1960 until 2020\n",
    "- [`life-expectancy.csv`](./data/life-exp.csv) – World Bank life expectancy estimates for world countries and regions from 1960 until 2019\n",
    "- [`m49.csv`](./data/m49.csv) – United Nations [M49](https://en.wikipedia.org/wiki/UN_M49) Standard Country or Area Codes for Statistical Use\n",
    "- [`population.csv`](./data/population.csv) – World Bank population estimates fror world coutneires and regions from 1960 until 2020\n",
    "\n",
    "All the datasets are in [RFC 4180 CSV](https://datatracker.ietf.org/doc/html/rfc4180) (comma-separated values) format and the first four rows of the World Bank data files contain metadata with the actual data table starting on row five.\n",
    "\n",
    "Let us start by reading in the population data. Pandas can easily read CSV datasets via the [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function. The function reads the contents of the file into a [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) data structure and supports various additional arguments. For example, we can utilize the `skiprows` argument to tell Pandas to skip the frist four rows of the dataset as the data table does not start until fow five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744b19c-db10-4f4f-ba7e-ad84069bfae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('data/population.csv', skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef3e90-bb5f-4904-8320-355c2b0d3a99",
   "metadata": {},
   "source": [
    "Now the World Bank population dataset is stored in a DataFrame called `population`. Calling the DataFrame by its name will display the first and last five rows of the table by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb99f7c-458a-4d9e-9d6e-309d6865bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc78cd-36a1-495f-95a3-5b84f73430a5",
   "metadata": {},
   "source": [
    "We see that the dataframe appears to have the following columns:\n",
    "\n",
    "- `Country Name` – English name of the country\n",
    "- `Country Code` – [ISO 3166-1 alpha-3](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3) country code\n",
    "- `Indicator Name` – name of the indicator represented by the data\n",
    "- `Indicator Code` – World Bank code for the indicator\n",
    "- `1960` ... `2020` – population estimates by year\n",
    "\n",
    "We also see that the DataFrame has 266 rows and 65 columns. We can double-check this by looking at the value of the `pandas.DataFrame.shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ed0a2-b870-42da-8342-1dcef2b2af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1278d0b-da95-4c83-b022-55926efba5b0",
   "metadata": {},
   "source": [
    "The `pandas.DataFrame.size` attribue will give us the total number of values in the table (number of columns times number of rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400dae1-baad-4298-bbad-91d08a985fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fd4d6-ab6c-4c78-8c15-2e9282d7c96e",
   "metadata": {},
   "source": [
    "`pandas.DataFrame.columns`can be used to get a list of all the column names and `pandas.DataFrame.dtypes` will display the datatype of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044c2bd-41aa-4a58-9788-d246df3ed18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d630ee-18fb-4e63-b6d9-0e87ad547e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd83f00-9526-4ecb-9780-f52e3560ec8f",
   "metadata": {},
   "source": [
    "Note how the first four columns all have the `object` datatype. This denotes either textual data (string) or a mixed datatype (like a list or some other data structure). The population columns are all `float64` denoting floating-point numbers. It might feel odd to store population values as floating-point numbers as population counts are always whole integers. However, in Pandas all numeric data is stored as floating-point numbers by default. This is due to the fact that integer columns in Pandas do not support missing data values ... yet. Currently the default missing data value in Pandas is the `numpy.nan` from NumPy.\n",
    "\n",
    "We know that the `population` DataFrame stores population values, so the `Indicator Name` and `Indicator Code` columns are redundant. We can drop them from the table using the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f8f00-a30c-46a0-b91a-47cc9437776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.drop(columns=['Indicator Name', 'Indicator Code'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8051da39-fd74-4ea4-b540-627e48284458",
   "metadata": {},
   "source": [
    "Note how we specified two arguments when calling the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method. Frist we specified a list of columns to drop using the `columns` argument. The [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method also supports dropping rows, so that is why the `columns` argument is needed. Then we also specified `inplace` to be `True`. This ensures that the original `population` DataFrame gets modified. Otherwise the method would just return a new DataFrame and keep the `population` dataframe unchanged.\n",
    "\n",
    "We can validate that the desired columns have been removed by taking a quick peen at the DataFrame via the [`pandas.DataFrame.head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method. It displayes the fist five rows of the dataframe by defaut but you can also pass the number of rows desired as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d1ba5-d0ae-4dfc-8d4b-948c3db2d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbb3ad-729e-4f52-af57-d75af9ee583c",
   "metadata": {},
   "source": [
    "Knowing that the World Bank GDP dataset follows the extact same format as the World Bank population dataset, we can read it in and drop the `Indicator Name` and `Indicator Code` columns all in one go by chaining together the [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function and the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method. If we want to include a line break somewhere in the cain, we need to wrap the whole thing in parentheses `()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651e2e0-25d0-4411-bc58-09cb7f7ec667",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = (pd.read_csv('data/gdp.csv', skiprows=4)\n",
    "         .drop(columns=['Indicator Name', 'Indicator Code']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65f229-415d-47d2-84c9-c7d66fb66d33",
   "metadata": {},
   "source": [
    "Note how here we did not specify `inplace=True` when dropping the columns. That is because we want the [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method to take the DataFrame generated by [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) and then output a new DataFrame that we can save into the `gdp` variable. We can take a look at our newly created DataFrame by using the [`pandas.DataFrame.head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c039a4-ad72-491f-95f0-6387b0283c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4d5a0-6900-4523-acf6-bb518347aa6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Long vs Wide Data\n",
    "\n",
    "GDP on its own is not a good indicator of a coutries wealth as contries with more people tend to have higher GDP. But if we were to normalize GDP by population, then the resulting GDP per capita values can be comared across countries and used as a proxy for wealth. To do so, we must be able to match up the GDP and population values for each unqiue combination of country and year.\n",
    "\n",
    "The GDP and population tables currently are in wide format – each row represent a unique country and each column represents a unique year with the cell values representing unique population estimates. While this wide format has many advantages and is commonly used in geospatial applications, it does compliate joining various datasets. One option would be to treat both tables and matrices and calculate GDP per capita via by deviding the GDP matric with the population matrix. However, both tables need to have the exact same layout with the same number of countries and years in the same exact order for this to work and the result to be relable. Ensuring this is not a trivial task, so this method would involve a lot of work to produce reliable results.\n",
    "\n",
    "Alternativeley the two tables could be joined by country. Then we will have an extra-wide table with two sets of year columns – one set of year columns for population and another set of year columns for GDP. Then we would need to create another new column for each year by dividing the corresponding GDP column with the corresponding population column, resulting in another new set of year columns. As you can see, this approach would quickly leed to a vary messy and difficult to manage dataset and would also involve a lot of work, making it far from preferred.\n",
    "\n",
    "The easiest option for calcluating GDP per capita would involve converting both datasets into a long format, whrere each row represents a single unique observation (estimation). Instead of having countries in rows and years in columns, each row would instead represent a unique country and year combination. This would allow us to easily combine datasets on both country and year, ensuring that the GDP and population values for each country-year combination get matched.\n",
    "\n",
    "We can use the [`pandas.DataFrame.melt()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html) method to convert wide datafames to long format. We need to specify three argument when using this method:\n",
    "\n",
    "- `id_vars` – name(s) of the column(s) that define an unique observation in the original wide dataset\n",
    "- `var_name` – name of the the column in the new long dataset that stores the column names of the original wide dataset\n",
    "- `value_name` – name of the column int he new long dataset that stores the values of the original wide dataset\n",
    "\n",
    "Each observation in the original wide dataset represents a unique country defined either by the country name or country code. Let us include both of these as `id_vars` to carry both columns over to the long dataset. The columns of the wide dataset represent years, so that is the name we will pass on to the `var_name` argument. The values of the wide dataset represent population estimates, so that will be the name passed on to the `value_name` argument.\n",
    "\n",
    "The reverse command for [`pandas.DataFrame.melt()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html) is [`pandas.DataFrame.pivot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html), which can convert a long format table to wide format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef976e-1d4b-4196-a939-29669bcd032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long = population.melt(id_vars=['Country Name', 'Country Code'],\n",
    "                                  var_name='year',\n",
    "                                  value_name='population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a15cd0-209e-4b8d-aa0b-31c82873409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6b7f4-7a30-4103-a6b6-ea4803dae590",
   "metadata": {},
   "source": [
    "Now we have a new long population DataFrame called `population_wide`, where each row represent an unique country and year combination. Let us use `pandas.DataFrame.dtypes` to confirm the data types of this new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01023410-99c9-4175-bbce-e9b4a11edfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f856994-6b7f-4de9-ba6f-60755b968b4a",
   "metadata": {},
   "source": [
    "Note how the `year` column is of type `object`, meaning that the years are currenty stored as strings. As the years were perviously column names, this make sense. However, as years are actually numbers, they should also be stored as such to allow for easy comparisons and mathematical operations.\n",
    "\n",
    "To convert the year values to integers, we must first extract the `year` column as a `pandas.Series` object. This can be done by either using square brackets `df[\"column\"]` or via dot-notation `df.column`. The latter requires the column name to consist of only letters, numbers, and underscores (and not start with a number), so it is only useful if the column names are neatly formatted. Using square bracets to extract columns is more robust and as the column name is passed as a string, it can contain spaces and other special characters.\n",
    "\n",
    "Square brackets can be used to also create a new column or overrite an existing column. Dot-notation should only be used to read columns. Attempting to write columns using dot-notation could have unexpected consenquences.\n",
    "\n",
    "Knowing this, let us extract the `year` column as a Series object using dot-notation `df.column` and then call [`pandas.Series.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html) on the extreacted values to convert them to integers. Then we can use square bracket notation `df[\"column\"]` to replace the values of the `year` column with their integer equivaletns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527d81e-8f90-41e0-b6f8-bbc78a253dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long['year'] = population_long.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa230d-36de-46f3-abf5-b6bb47a04c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e985dd4-4562-44a1-9f71-8ac37497b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_long.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f211b-c738-44d2-b5ab-9e2ff7d7dba5",
   "metadata": {},
   "source": [
    "Note how the values of the `year` column seemingly did not change, but the datatype of the values is now `int32`, which means that the values have been converted to numeric integers.\n",
    "\n",
    "Now let us convert the GDP dataset to long format as well. We can chain the [`pandas.DataFrame.melt()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html) method together with the [`pandas.DataFrame.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) method to convert the DataFrame from wide to long format and change the datatype of the `year` column to integer all in one go. The [`pandas.DataFrame.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) method is very similar to the [`pandas.Series.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html) method, but instead of taking a single datatype as an argument, it takes a dictionary that maps column names to datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e487f-1644-4748-a40c-b6f42cf75638",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long = (gdp.melt(id_vars=['Country Name', 'Country Code'],\n",
    "                    var_name='year',\n",
    "                    value_name='gdp')\n",
    "               .astype({'year': int}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b346e5-d428-4205-81bb-a41edaa691da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f779178-e5c3-440c-9a78-8f05979251eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb55ecd-a0b0-4883-a0a3-6ad43db99567",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Joining Datasets\n",
    "\n",
    "Finally we are ready to combine the population and GDP datasets. [`pandas.DataFrame.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) can be used to perform a join on on one or more columns. The method is called on the left DataFrame and takes the right DataFrame as its first argument (this is only important to know when performing a left or right join). Additional arguments are as follows:\n",
    "\n",
    "- `on` – A single column name (string) or list of column names to join on. These column names should appear in both tables. If the column names differ between datsets, the seperate `left_on` and `right_on` arguments should be used isntead.\n",
    "- `how` – The type of join to perform. Here are the possible values:\n",
    "    - `\"left\"` – use only keys from the left DataFrame (include all rows from left DataFrame)\n",
    "    - `\"right\"` – use only keys from the right DataFrame (include all rows from right DataFrame)\n",
    "    - `\"outer\"` – use the union of keys from both DataFrames (include all rows from both DataFrames)\n",
    "    - `\"inner\"` – use the isntersection of keys from both DataFrames (include only matching rows)\n",
    "    - `\"cross\"` – creates the cartesian product from both DataFrames (similar to cross-tabulation)\n",
    "\n",
    "We would like to join on each unique country and year combination. As spellings of country names might differ between datasets, it is good practice to always use the [ISO 3166-1 alpha-3](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3) country code or some other analogous unique identifier to distinquish between countries. The country code for each country is determined by an international standard and should not differ between datasets, allowing us to reliably join the data. Hece we will specify `on=[\"Country Code, \"year\"]` to perform the join on unique country-year combinations and `how=\"inner\"` to only keep year-country combinations that are present in both datasets. Since we do not want the `Country Name` column repeated in the joined dataset, we should remove it from the GDP table using [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) before performing the join. Otherwise the `Country Name` column from the GDP daset will also get joined, resulting in the joined table having two seperate columns with country names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1851e-3f21-4e85-a128-cf73f7fd78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = population_long.merge(gdp_long.drop(columns='Country Name'),\n",
    "                             on=['Country Code', 'year'],\n",
    "                             how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af607d13-d44c-4a72-9909-3ef9f26762c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ad2d3-fcc8-4ea6-b21a-34e76d054edb",
   "metadata": {},
   "source": [
    "Now we have a table with a population and GDP value for each country and year combination. We can easily add a new column denoting GDP per capita to this table by dividng the GDP column with the population column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb782928-c205-4756-b3da-8efc1eef11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gdp_per_capita'] = data.gdp / data.population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6ff2c-5274-4948-afa7-d8ba036ee3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c1d48-2e25-4dc5-9fa4-dbaf95273f17",
   "metadata": {},
   "source": [
    "Now we would also like to add life expectancy information to this joined dataset. Knowing that all World Bank data tables follow the same format, we can easily convert the worflow from before into a function that reads in a World Bank datset, drops unneeded columns, converts it to long format, and ensure the year is in numeric format. That function would only need two inputs – the path of the CSV file and the name of the indicator represented by the data. (This name will be used as the colum name for the values column in the long format table.) Let us define this function and use it to read in the World Bank life expectancy dataset and convert it to long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f007d-e38d-4577-a9ea-b630abe30d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_world_bank_data(file_name, value_name):\n",
    "    return (pd.read_csv(file_name, skiprows=4)\n",
    "              .drop(columns=['Indicator Name', 'Indicator Code'])\n",
    "              .melt(id_vars=['Country Name', 'Country Code'],\n",
    "                    var_name='year',\n",
    "                    value_name=value_name)\n",
    "              .astype({'year': int}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a8150-6632-4ea9-8102-664c8da30533",
   "metadata": {},
   "outputs": [],
   "source": [
    "life_exp = read_world_bank_data(file_name = 'data/life-expectancy.csv',\n",
    "                                value_name = 'life_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90450b9e-194c-4692-af9b-6c59eaa0982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "life_exp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45a8fc-4824-422d-88d8-0e2c23a1d966",
   "metadata": {},
   "source": [
    "Using the same workflow from before, we can join the long format life expectancy dataset to our table containing the GDP and population data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2207d-a7e2-4396-b7de-bb60898e9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(life_exp.drop(columns='Country Name'),\n",
    "                  on=['Country Code', 'year'],\n",
    "                  how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb161df-eb0b-4004-beed-be5c56fd2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ba039-7bd4-4db4-82b1-2d229befe981",
   "metadata": {},
   "source": [
    "Finally we would also like to know which [United Nations regional geoscheme](https://en.wikipedia.org/wiki/United_Nations_geoscheme) the country belongs to. Information on this is availalbe in the United Nations [M49](https://en.wikipedia.org/wiki/UN_M49) dataset. As this dataset is a  standard CSV table, we can use [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) without any additional arguments to read it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb26b7e-00f4-45eb-9ffe-a9bf38712618",
   "metadata": {},
   "outputs": [],
   "source": [
    "m49 = pd.read_csv('data/m49.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406a831-faff-47a4-9a13-b697480f8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "m49.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff49e5e-2e16-4d04-aa03-8930daa910e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m49.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faac402-25aa-4d27-b052-08252eeedd8e",
   "metadata": {},
   "source": [
    "Note how this dataset contains a lot of information on the various groups and codes assigned to each country. We are only interested in the name of the region the country belongs into and the [ISO 3166-1 alpha-3](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3) code assigned to the country. Using double squre brackets `[[ ]]` we can extract the desired columns as a new DataFrame. (In reality we are just passing a list of column names to the standard single square brackets indexer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912b5c6-2b40-42e1-b884-c650a100a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = m49[['Region Name', 'ISO-alpha3 Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52444f-2ea5-4326-8e4e-52c06fd92883",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fdbe7-202d-44f8-aed0-d5bfac8bbdaf",
   "metadata": {},
   "source": [
    "Now we can use [`pandas.DataFrame.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) again to join the regions to the rest of our data. Since the names of the columns containing the country code information differ between the datasets, we must use the `left_on` and `right_on` arguments instead of the `on` argument from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e61add-145d-4828-bb31-8a449f638014",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(regions,\n",
    "                  left_on='Country Code',\n",
    "                  right_on='ISO-alpha3 Code',\n",
    "                  how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80a646-84b5-4ccb-bfb7-60a8a80e9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46487b19-4713-46e9-8445-19e791d7af67",
   "metadata": {},
   "source": [
    "Note how the new joined datset contains both of the country code columns (because their names were different). Also, the naming convetion in our table is not uniform – some column names are in [`snake_case`](https://en.wikipedia.org/wiki/Snake_case) (which is preferred) while others contain spaces and a mix of uppercase and lowercase letters. Let us use [`pandas.DataFrame.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) to drop the second country code column and [`pandas.DataFrame.rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) to rename some of the colums to ensure an uniform column naming convention. Remember that we can use the `inplace=True` argument to apply the changes to the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977f2f9-591b-4bff-8687-ff7fb8b24266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='ISO-alpha3 Code', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6832f-063c-4d58-b1b5-c694a7df7f0c",
   "metadata": {},
   "source": [
    "[`pandas.DataFrame.rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) takes a dictionary in the format `{\"old_name\": \"new_name\"}` as an argument and you need to specify wheter you would like to rename rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203d5f9-8b2d-41b7-8872-8962ce685e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'Country Name': 'country_name',\n",
    "                     'Country Code': 'country_code',\n",
    "                     'Region Name': 'region_name'},\n",
    "            inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96b445-0c28-40f3-963b-877504157146",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8d8a2-8bb5-4c9c-8fc2-920cf9993307",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Boolean Indexing\n",
    "\n",
    "To extract specific rows from a DataFrame, we can combine the square brackets indexing operator `pandas.DataFrame[]` with a logial operation that produces a boolean array. This would select every row from the DataFrame where the cooresponding element in the boolean array equals `True`. For example, to extact all rows that correspond to the United States, we could use `data.country_code == \"USA\"`. This would return an array of `True` and `False` values where the value of a specific element in the array is `True` if the corresponding row in the `data` DataFrame had the value `\"USA\"` in its `country_code` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83974b-9da0-47d7-b200-4f9a97a358c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.country_code == 'USA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba1d21-9875-4333-b79c-14f639386b91",
   "metadata": {},
   "source": [
    "Combining this with the square brackets indexing operator `data[]` will extract all values from the `data` DataFrame where the `country_code` column has the value `\"USA\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829e274-d5b8-4af2-9d86-7f1b789eec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data = data[data.country_code == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c72ad-4d05-44a3-bf8d-ceb17f64ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5601cda-ff1a-4a5e-88df-52b8cfa2a2a7",
   "metadata": {},
   "source": [
    "We can ensure that this new `usa_data` DataFrame only contains values corresponding to the United States by calling [`pandas.Series.unique()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) on the `country_name` column. This will return an array of all the uniqe country names present in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e41588-11a3-4b31-99a0-44b9f2fbdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.country_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38319a58-57de-4901-9506-29eecf4e36e4",
   "metadata": {},
   "source": [
    "Note taht even though there is only one unique value, the result is still an array. To extract the value as a string, we must extract the first element of the array using `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea72f0e-d443-4002-8b19-e3f76889a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.country_name.unique()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab182b00-2284-4a59-aceb-ced28829f8db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating Static Line Graphs\n",
    "\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/) is the primary plotting library in Python and it is designed to resemble the plotting functionalities of MATLAB. While it provides all kinds of different plotting functionality, the [`matplotlib.plyplot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html) module is used the most. It is common to import this module under the alias `plt` as we did before. Matplotlib works in a layered fashion. First you define your plot using [`matplotlib.pyplot.plot(x, y, ...)`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html), then you can use additional [`matplotlib.pyplot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html) methods to add more layers to your plot or modify its appearance. Finally, you use [`matplotlib.pyplot.show()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) to display the plot or [`matplotlib.pyplot.savefig()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) to save it to an external file.\n",
    "\n",
    "The `x` and `y` arguments in the [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) call can be either arrays or [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) objects. For example, we can visualize the population of the United States over time by extracting the `year` and `population` columns of the `usa_data` table as [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) objects and passing them along to [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb4e4d-e062-4e1b-af12-62f69ddbb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(usa_data.year, usa_data.population)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c09ca5-d0cc-422e-afce-31666748d781",
   "metadata": {},
   "source": [
    "Alternativley we could pass the [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to the [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) command using the optional `data` argument. This will allow us to specify the desired column names as the `x` and `y` arguments instead of having to extract them as [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) objects. For example, we can visualize the GDP of the United States over time as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f63f3-970d-4b31-9bae-80fed17b9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot('year', 'gdp', data=usa_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5138981-9234-47b6-a1e3-44aba7736b76",
   "metadata": {},
   "source": [
    "[Pandas](https://pandas.pydata.org/) also has built-in plotting fucntinality via the [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) method. It takes the column names of the `x` and `y` columns as arguments and uses a plotting backend to generate the plot. By default, the plotting backend is Matplotlib, but this could be reconfigured to be something else instead. For example, we can create a Matplotlib visualization showing United States life expectancy over time as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd45aeb-c551-4ae9-8685-e8dcf8f63e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data.plot(x='year', y='life_exp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879be18-b111-4772-b7cd-4c3981b4b30e",
   "metadata": {},
   "source": [
    "To create a line graph with multiple lines, we need to stack the lines using mutiple [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) or [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) calls. But how can we specify that we would like to stack the lines onto a signle plot instead of creating a new plot for each line? This is where the [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) class comes into play. For simplicity, you can think of each [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object as a canvas onto which one can add multiple layers of visualization. When using [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) to create visulaiztations, we can utilize [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) to create multi-layered plots as follows:\n",
    "1. The first [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) command will return a [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object. This object should be saved into a variable. It is common to save it into a varaible called `ax`.\n",
    "2. In each subsequent [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) call, the [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object from before should be passed on using the `ax` argument. This will ensure the new plot gets added to the same canvas.\n",
    "\n",
    "We can combine the [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) call with boolean indexing to easily visualize subsets of the data and use the `color` and `label` arguments to specify a color and legend label for each subset.\n",
    "\n",
    "Once all the lines have been added to the plot, we can use [`matplotlib.pyplot.ylabel()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html) and [`matplotlib.pyplot.xlabel()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html) to label the axes and [`matplotlib.pyplot.title()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.title.html) to specify a title for the visualization. Finally we call [`matplotlib.pyplot.show()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) to display the plot.\n",
    "\n",
    "Knowing all this, a visualization illustrating the GDP per capita over time for North American countries can be generated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c465db-3fdf-4b94-9cd4-553ab2c9b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data[data.country_code == 'USA'].plot(x='year',\n",
    "                                           y='gdp_per_capita',\n",
    "                                           color='blue',\n",
    "                                           label='USA')\n",
    "\n",
    "data[data.country_code == 'CAN'].plot(x='year',\n",
    "                                      y='gdp_per_capita',\n",
    "                                      color='red',\n",
    "                                      label='Canada',\n",
    "                                      ax=ax)\n",
    "\n",
    "data[data.country_code == 'MEX'].plot(x='year',\n",
    "                                      y='gdp_per_capita',\n",
    "                                      color='green',\n",
    "                                      label='Mexico',\n",
    "                                      ax=ax)\n",
    "\n",
    "plt.ylabel('GDP per capita')\n",
    "plt.xlabel('Year')\n",
    "plt.title('GDP per Capita Over Time for North American Countries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c70f4-973e-45b2-855c-ba9a420d34bc",
   "metadata": {},
   "source": [
    "There are many benefits to using [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) over [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) when dealing with DataFrames. Most importantly, [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) interacts directly with a Pandas DataFrame and has a much simpler user interface with numerous named arguments allowing for easy costumization. Hoewever, when it comes to more advanced tasks, Matplotlib allows for better fine-tuning and more flexibility. However, this comes at a cost of more complex commands. For example, to create a plot display the temporal variation of both United States life expectancy and GDP per capita using two different y-axes, we must use a relativley advanced workflow.\n",
    "\n",
    "First, we define the size of our plot using the `figsize` argument of [`matplotlib.pyplot.subplots()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html). This command allows for the creation of multiple subplots, but most frequently it is used to specify the size of a single plot. It returns a tuple consisting of a [`matplotlib.figure.Figure`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure) and a [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object.\n",
    "\n",
    "To add a plot layer to a specific [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object, we can use [`matplotlib.axes.Axes.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html) which works very similarly to the previously discussed [`matplotlib.pyplot.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) command. Both commands take an optional format string as the third positional argument that allows you to specify the line and marker style and color using a simple shorthand. For example, `\"g--\"` means a green dashed line and `\"mx\"` indicates magenta-colored x-shaped markers. Refer to the function documentation for a fill overview of all the shorthand characters. The Matplotlib commands for adding axes labels and plot titles also have additional arguments that modify the appearance of the label or title. For example, `color` usually specifies the text color and `size` is used to specify the size of the font.\n",
    "\n",
    "To add another y-axis to the plot, we can use [`matplotlib.axes.Axes.twinx()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.twinx.html) to create another [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#the-axes-class) object that defines a new y-axis but shares the same x-axis.\n",
    "\n",
    "Finally, we can use [`matplotlib.figure.Figure.legend()`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.legend) to add a legend to the whole figure (including all the axes objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ce090-ea0a-46ec-a501-2e1eb6be20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(usa_data.year, usa_data.gdp_per_capita, 'g--', label='GDP per Capita')\n",
    "plt.ylabel('GDP per Capita', color='g',)\n",
    "plt.xlabel('Year')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(usa_data.year, usa_data.life_exp, 'mx', label='Life Expectancy')\n",
    "plt.ylabel('Life Expectancy', color='m')\n",
    "plt.title('United States', size=20)\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e8b7d-857a-41c9-b63b-d902f580a660",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizing Distributions and Correlations\n",
    "\n",
    "Let us return to our original goal of exploring the relationship between health and wealth. We will use GDP per capita as a proxy for wealth and life expectancy as an inidicator of health. We can simplify the analysis by looking only at one point in time and focus our analysis on 2019, which is the latest year we have both GDP per capita and life ecpectancy data availalbe. We shall use boolean indexing to extract 2019 data into a new DataFrame called `data2019`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee29a7-98bc-4a80-877e-d0b79a1c50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019 = data[data.year == 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a818751-0b83-420b-af6d-6619a1d55f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d94f1-df8b-4e4d-9740-b9f96323a93f",
   "metadata": {},
   "source": [
    "How is wealth distributed amongst the global population? Let us get a vague idea by visualizing the distribution of GDP per capita amongst world countries in 2019. We can easily create an histogram by using the [`matplotlib.pyplot.hist()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) command and passing it the GDP per capita [`pandas.Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4f1b5-ad97-4c37-b10b-e489aa0ebccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data2019.gdp_per_capita)\n",
    "plt.xlabel('GDP per Capita')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532515d1-c811-4424-88dd-a378d8ad9a72",
   "metadata": {},
   "source": [
    "Note how we were able to easily create a historgram, but the result was quite ugly. If we wanted a prettier plot, we could go though the trouble of customizing the plot using various additional arguments and commands, which would take quite a while. Or we could use [Seaborn](https://seaborn.pydata.org/) which allows us to easly create beautiful visualizations with sensible defaults. For example, we could create a well-designed histogram with a smoothed kernel density esimtate (KDE) overlay using the [`seaborn.histplot()`](https://seaborn.pydata.org/generated/seaborn.histplot.html) function along with the `kde=True` flag. Knowing this, let us look at the distribution of life expectancy amongst world countries in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f15bd-0f29-4881-bad2-462383fe05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data2019.life_exp, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efd341-f157-47b4-bcba-16ffffffd1af",
   "metadata": {},
   "source": [
    "To easily create a scatterplot analyzing the relationship between GDP per capita and life expactancy, we can use [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) and specify `kind=\"scatter\"` to ensure the result is a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bff821-140e-4b3f-8a6e-808f9e58ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019.plot(x='gdp_per_capita', y='life_exp', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51cf70f-7fb9-4a76-91e6-289120d3d320",
   "metadata": {},
   "source": [
    "The relationship appears to be logarithmic. This is likely due to the distribution of GDP per capita being heavily skewed. We can easily confirm this by plotting a two-dimensional kernel density estimate (KDE) plot [`seaborn.jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) along with `kind=\"kde\"`. (To get a scatterplot with histograms, one would use `kind=\"scatter\"`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32205a2c-d451-416e-9652-a07b65f0cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=data2019,\n",
    "              x='gdp_per_capita',\n",
    "              y='life_exp',\n",
    "              kind='kde',\n",
    "              fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09348-08d0-4fb0-babe-05996dbb4169",
   "metadata": {},
   "source": [
    "To get a better sense of the potentially-logarithmic relationship between GDP per capita and life expectancy, we should apply a logarithmic transformation to the axis corresponding to GDP per capita. In our exaple this is the x-axis and we can apply a logarithmic transofrmation on the x-axis by passing `\"log\"` to the [`matplotlib.pyplot.xscale()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xscale.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82fa13-adcf-40a6-a37e-78997c3a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data2019.gdp_per_capita, data2019.life_exp)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('GDP per Capita')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd717f54-a731-4deb-9d6c-85cd84c397d9",
   "metadata": {},
   "source": [
    "Note how we used [`matplotlib.pyplot.scatter()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) instead of [`pandas.DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) to create the scatterplot. Both functions are very similar and in reality, the latter simply calles the former. Also note how now the x-axis of the scatterplot is logarithmic. This makes the relationship much clearer and we can quite definitely state that there appears to be a logarithmic relationship beween life expectancy and GDP per capita.\n",
    "\n",
    "But does the size of a country play a role in this relationsip? To find out, we can scale the size of the data points poroportionally to the population such that bigger points indicate countries with more population. This can be done using the `s` argument in [`matplotlib.pyplot.scatter()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html), which takes an array of point sizes. This array needs to be the same size as the `x` and `y` arrays with one size value for each `x` and `y` combination. We can easily generate an array like this using the formula $X \\div max(X) \\times s$ where $X$ is the array we want to base the sizes on and $s$ is a scaling factor in arbritary plot units. Note that the scaling factor is completely arbritary and you might need to try different values until you find something that makes the visualization look good. We divide the input array with its maximum value to properly normalize and scale the sizes.\n",
    "\n",
    "Scaling the point sizes by population might cause some bigger points to overlap smaller ones. To ensure we can properly see overlapping points, we can use the `alpha` argument in the [`matplotlib.pyplot.scatter()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) call to specify a transparency factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a3e64-664d-48ad-9750-a55f715a222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(data2019.gdp_per_capita,\n",
    "            data2019.life_exp,\n",
    "            s=data2019.population/data2019.population.max()*5000,\n",
    "            alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('GDP per Capita')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afc9cd-0a48-4dee-9d23-23d9794c26e5",
   "metadata": {},
   "source": [
    "Looks like the size of a country does not seem to be related to GDP per capita or life expectancy. But what about the region a country is in? There is a good chanche a correlation exists between the geographical location of a country and other indicators. To find out, we should color the points based on their geographic region. We know from before that this requires adding multiple layers to the plot – one for each region. We can get a list of all the regions by using [`pandas.Series.unique()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) on the `region_name` column. Then we can iterate over that list using a loop, subset the data for each region, and create a scatterplot layer using the subsetted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88b77a-e948-40f5-82f9-53bae07aac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for region in data2019.region_name.unique():\n",
    "    \n",
    "    subset = data2019[data2019.region_name == region]\n",
    "    \n",
    "    ax.scatter(subset.gdp_per_capita,\n",
    "                subset.life_exp,\n",
    "                s=subset.population/data2019.population.max()*5000,\n",
    "                label=region,\n",
    "                alpha=0.5)\n",
    "    \n",
    "plt.xscale('log')\n",
    "plt.xlabel('GDP per Capita')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.title('2019')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f796d-7ea1-443d-b624-ef74bb97f77a",
   "metadata": {},
   "source": [
    "One of the main drawbakcs on Matplotlib is the fact that one needs to create mutiple layers to visualize groups using different colors. This can be a tedious process and usually involves having to subset the data using a loop. To circuimnavigate this, many choose to use Seaborn instead, which allows for a grouping variable to be passed via the `hue` argument. For example, to recreate the plot from above without having to use a loop, we can utilize [`seaborn.scatterplot()`](https://seaborn.pydata.org/generated/seaborn.scatteplot.html) with `hue=\"region_name\"`. To scale the point sizes by populaton, we can specify `size=\"population\"` then use the `sizes` arguments to give a tuple that defines the smallest point size and the largest point size in arbritary plot units. As before, you might need to play around with the tuple values in `sizes` until you find a combination that looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dff6a9-b474-4ace-846a-007667ec1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(data=data2019,\n",
    "                x='gdp_per_capita',\n",
    "                y='life_exp',\n",
    "                hue='region_name',\n",
    "                size='population',\n",
    "                sizes=(10, 5000),\n",
    "                alpha=0.5,\n",
    "                legend=False,\n",
    "                ax=ax)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('GDP per Capita')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.title('2019')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6a7b9-211b-4584-b436-42c6f06cda92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating Interactive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405796e-fef1-493f-8d5a-ec3b24b48e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019.hvplot.scatter(x='gdp_per_capita',\n",
    "                        y='life_exp',\n",
    "                        s='population',\n",
    "                        c='region_name',\n",
    "                        scale=1/data2019.population.max()*2000000,\n",
    "                        hover_cols=['country_name', 'country_code'],\n",
    "                        alpha=0.5,\n",
    "                        logx=True,\n",
    "                        width=650,\n",
    "                        height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d2513-9c64-4c80-ba8a-85b0d62d44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=data2019.dropna(),\n",
    "           x='gdp_per_capita',\n",
    "           y='life_exp',\n",
    "           size='population',\n",
    "           color='region_name',\n",
    "           hover_name='country_name',\n",
    "           hover_data=['country_code'],\n",
    "           size_max=40,\n",
    "           opacity=0.5,\n",
    "           log_x=True,\n",
    "           width=650,\n",
    "           height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0d1ba-1b6d-4f08-b233-ea5b7b21c729",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Working with Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da825e69-21a8-48db-9d35-8e1b44d4a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta = pd.read_csv('data/mbta-gated-entries-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2f5e4-12a5-412a-af9b-2a25a8ba4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3ff4e-6118-4765-87d6-7ed05fdb8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de97266-29eb-4636-8c39-2c2a9598d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta['time_period'] = mbta.time_period.str.strip('()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3ba7c-e7f7-4d43-b364-00123c17eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689037a-1c17-4317-a337-29173cc64e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta['timestamp'] = pd.to_datetime(mbta.service_date + ' ' + mbta.time_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d30c8e-4d84-4e60-b420-a8a3b9fcc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c582dd-a0b3-4d89-a543-fb0559f8431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a4df6-fc0c-42a3-a8a3-96f5cd9431ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta = mbta[['timestamp', 'station_name', 'route_or_line', 'gated_entries']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f367cec-b28c-4612-b69e-6b9f64621278",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1ddf0-1ae6-4581-aa12-24846e6a8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45560b-70d1-4203-98e7-5189c6bf246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[mbta.timestamp == '2020-02-24'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfdbb7-de9a-4747-b81e-bd29a64ac356",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[\n",
    "    (mbta.timestamp >= '2020-02-01') & (mbta.timestamp < '2020-03-01')].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ce330-d90f-4c9f-ae4c-ab204ce4b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta.gated_entries[mbta.timestamp.dt.month == 2].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d10ecc-8398-4a68-b6f1-e152e13c622b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27e06a-1982-4eb2-ba1a-d44e869f6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta['date'] = mbta.timestamp.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155aa25-5e43-4b05-ab3a-0cf14711c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_agg = mbta.groupby('date').gated_entries.sum().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f3f24-7bb8-4927-86be-641ee7cb388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ece1e3-1e99-49c8-bbcc-12b62df0ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_agg.gated_entries.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a12ce-b116-4391-a131-98b3c5f7ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_agg.date[mbta_agg.gated_entries == mbta_agg.gated_entries.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4ee24-7963-4fc2-b321-3e16c4db0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_agg.date[mbta_agg.gated_entries == mbta_agg.gated_entries.max()].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606621c4-0f55-4ec3-a972-505f8ba53bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mbta.groupby('station_name')\n",
    "     .gated_entries.sum()\n",
    "     .sort_values()\n",
    "     .to_frame()\n",
    "     .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12bafe-a9a4-4e23-9b35-bc6dc88c7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mbta.groupby('route_or_line')\n",
    "     .gated_entries.sum()\n",
    "     .sort_values()\n",
    "     .to_frame()\n",
    "     .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e0338-5411-4e83-8e21-32a7b949e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_bydate = (mbta.groupby(['date', 'station_name', 'route_or_line'])\n",
    "                   .gated_entries.sum()\n",
    "                   .to_frame()\n",
    "                   .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1d4c7-779e-4fe8-92e2-192c0d361dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbta_bydate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5262d-2825-4d89-8618-c811ef2eb1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
